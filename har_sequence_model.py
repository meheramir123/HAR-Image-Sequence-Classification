# -*- coding: utf-8 -*-
"""har_sequence_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14IWY_W-F2YerK5x6AVcNbAjWSoTxAq0J
"""

!unzip /content/archive.zip -d /content/

import os
import argparse
import numpy as np
import pandas as pd
import cv2
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
import joblib

RANDOM_SEED = 42
IMG_SIZE = (64, 64)
CHANNELS = 1  # Grayscale
SEQUENCE_LENGTH = 5
BATCH_SIZE = 16
EPOCHS = 30
LR = 1e-3

tf.random.set_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

def load_image_sequences_from_labels(df, data_folder, seq_len=SEQUENCE_LENGTH, img_size=IMG_SIZE, channels=CHANNELS):
    X, y, filenames_seq = [], [], []
    grouped = df.groupby('label')
    for label, group in grouped:
        group_sorted = group.sort_values('filename')
        filenames = group_sorted['filename'].tolist()
        for i in range(len(filenames) - seq_len + 1):
            frames = []
            seq_files = []
            for j in range(seq_len):
                img_path = os.path.join(data_folder, filenames[i+j])
                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE if channels==1 else cv2.IMREAD_COLOR)
                if img is None:
                    break
                img = cv2.resize(img, img_size)
                if channels == 1:
                    img = img[..., np.newaxis]
                frames.append(img)
                seq_files.append(filenames[i+j])
            if len(frames) == seq_len:
                X.append(np.array(frames, dtype='float32') / 255.0)
                y.append(label)
                filenames_seq.append(seq_files)
    return np.array(X), np.array(y), filenames_seq

def load_image_sequences_unlabeled(df, data_folder, seq_len=SEQUENCE_LENGTH, img_size=IMG_SIZE, channels=CHANNELS):
    X, filenames_seq = [], []
    filenames = df['filename'].tolist()
    for i in range(len(filenames) - seq_len + 1):
        frames = []
        seq_files = []
        for j in range(seq_len):
            img_path = os.path.join(data_folder, filenames[i+j])
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE if channels==1 else cv2.IMREAD_COLOR)
            if img is None:
                break
            img = cv2.resize(img, img_size)
            if channels == 1:
                img = img[..., np.newaxis]
            frames.append(img)
            seq_files.append(filenames[i+j])
        if len(frames) == seq_len:
            X.append(np.array(frames, dtype='float32') / 255.0)
            filenames_seq.append(seq_files)
    return np.array(X), filenames_seq

def plot_confusion_matrix(y_true, y_pred, class_names, save_path=None):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8,6))
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')
    plt.ylabel('True')
    plt.xlabel('Predicted')
    plt.title('Confusion Matrix')
    if save_path:
        plt.savefig(save_path)
    plt.show()

def plot_training_history(history, save_dir):
    # Loss
    plt.figure()
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title('Training Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.savefig(os.path.join(save_dir, 'loss_plot.png'))
    plt.show()

    # Accuracy
    plt.figure()
    plt.plot(history.history['accuracy'], label='Train Acc')
    plt.plot(history.history['val_accuracy'], label='Val Acc')
    plt.title('Training Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.savefig(os.path.join(save_dir, 'accuracy_plot.png'))
    plt.show()

def build_temporal_cnn_model(input_shape, num_classes):
    model = models.Sequential()
    model.add(layers.TimeDistributed(layers.Conv2D(32, (3,3), activation='relu', padding='same'), input_shape=input_shape))
    model.add(layers.TimeDistributed(layers.MaxPooling2D((2,2))))
    model.add(layers.TimeDistributed(layers.Conv2D(64, (3,3), activation='relu', padding='same')))
    model.add(layers.TimeDistributed(layers.MaxPooling2D((2,2))))
    model.add(layers.TimeDistributed(layers.Conv2D(128, (3,3), activation='relu', padding='same')))
    model.add(layers.TimeDistributed(layers.MaxPooling2D((2,2))))
    model.add(layers.TimeDistributed(layers.Flatten()))
    model.add(layers.LSTM(128, return_sequences=False))
    model.add(layers.Dropout(0.5))
    model.add(layers.Dense(num_classes, activation='softmax'))

    model.compile(optimizer=optimizers.Adam(learning_rate=LR),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

def main(args):
    os.makedirs(args.model_dir, exist_ok=True)

    train_csv = pd.read_csv(args.train_csv)
    test_csv = pd.read_csv(args.test_csv)

    # Load train data
    X_train, y_train, filenames_train_seq = load_image_sequences_from_labels(train_csv, args.data_folder)
    le = LabelEncoder()
    y_train_enc = le.fit_transform(y_train)
    class_names = list(le.classes_)

    # Train/val split
    X_train_final, X_val, y_train_final, y_val, filenames_train_final_seq, filenames_val_seq = train_test_split(
        X_train, y_train_enc, filenames_train_seq, test_size=0.2, random_state=RANDOM_SEED, stratify=y_train_enc
    )

    # Build model
    input_shape = X_train_final.shape[1:]
    model = build_temporal_cnn_model(input_shape, len(class_names))
    model.summary()

    es = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

    # Train
    history = model.fit(X_train_final, y_train_final,
                        validation_data=(X_val, y_val),
                        epochs=EPOCHS,
                        batch_size=BATCH_SIZE,
                        callbacks=[es])

    # Save training plots
    plot_training_history(history, args.model_dir)

    y_val_pred_seq = np.argmax(model.predict(X_val), axis=1)
    y_val_pred_labels_seq = le.inverse_transform(y_val_pred_seq)
    y_val_true_labels_seq = le.inverse_transform(y_val)

    # Aggregate per original video/file
    final_val_preds = {}
    final_val_true = {}
    for seq_files, true_label, pred_label in zip(filenames_val_seq, y_val_true_labels_seq, y_val_pred_labels_seq):
        for f in seq_files:
            if f not in final_val_preds:
                final_val_preds[f] = []
                final_val_true[f] = true_label
            final_val_preds[f].append(pred_label)

    # Majority vote per file
    y_val_pred_agg = [max(set(labels), key=labels.count) for labels in final_val_preds.values()]
    y_val_true_agg = [v for v in final_val_true.values()]

    # Confusion matrix and classification report
    plot_confusion_matrix(y_val_true_agg, y_val_pred_agg, class_names,
                          save_path=os.path.join(args.model_dir, 'confusion_matrix.png'))

    report = classification_report(y_val_true_agg, y_val_pred_agg, target_names=class_names, output_dict=True)
    df_report = pd.DataFrame(report).transpose()
    df_report.to_csv(os.path.join(args.model_dir, 'classification_report.csv'))
    print("Validation classification report saved.")

    model.save(os.path.join(args.model_dir, "temporal_cnn_har_model.keras"))
    joblib.dump(le, os.path.join(args.model_dir, "label_encoder.joblib"))
    print("Model and label encoder saved.")

    X_test, filenames_seq = load_image_sequences_unlabeled(test_csv, args.data_folder)
    y_test_pred_enc = np.argmax(model.predict(X_test), axis=1)
    y_test_pred_labels = le.inverse_transform(y_test_pred_enc)

    # Aggregate per file
    final_test_preds = {}
    for seq_files, label in zip(filenames_seq, y_test_pred_labels):
        for f in seq_files:
            if f not in final_test_preds:
                final_test_preds[f] = []
            final_test_preds[f].append(label)
    df_test_pred = pd.DataFrame([(f, max(set(labels), key=labels.count)) for f, labels in final_test_preds.items()],
                                columns=['filename', 'predicted_label'])
    df_test_pred.to_csv(os.path.join(args.model_dir, "test_predictions.csv"), index=False)
    print("Test predictions saved.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--train_csv", type=str, default="/content/Human Action Recognition/Training_set.csv")
    parser.add_argument("--test_csv", type=str, default="/content/Human Action Recognition/Testing_set.csv")
    parser.add_argument("--data_folder", type=str, default="/content/Human Action Recognition/train")
    parser.add_argument("--model_dir", type=str, default="/content/har_model_temporal")
    args, unknown = parser.parse_known_args()
    main(args)